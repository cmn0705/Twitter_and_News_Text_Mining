{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tweets\n",
    "Twitter API: https://developer.twitter.com/\n",
    "\n",
    "Python-Twitter: https://python-twitter.readthedocs.io/en/latest/\n",
    "\n",
    "Need a Twitter account and get its credential keys.\n",
    "\n",
    "Install: pip install python-twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the twitter.Api with an account Credentials\n",
    "import pandas\n",
    "import twitter\n",
    "\n",
    "df=pandas.read_csv('Twitter API Credentials.csv') #hide my credentials in this file\n",
    "api = twitter.Api(consumer_key=df.consumer_key[0],\n",
    "                    consumer_secret=df.consumer_secret[0],\n",
    "                    access_token_key=df.access_token_key[0],\n",
    "                    access_token_secret=df.access_token_secret[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to use twitter.Api**\n",
    "\n",
    "https://python-twitter.readthedocs.io/en/latest/twitter.html\n",
    "\n",
    "twitter.Api.GetSearch(term=None, raw_query=None, geocode=None, since_id=None, max_id=None, until=None, since=None, count=15, lang=None, locale=None, result_type='mixed', include_entities=None, return_json=False)\n",
    "\n",
    "***Note:***\n",
    "- **Cannot** pull more than 100 tweets at a time\n",
    "- **Cannot** pull tweets older than 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Status(ID=1388644383797485578, ScreenName=thundervolt888, Created=Sat May 01 23:59:55 +0000 2021, Text='RT @Wario64: Animal Crossing: New Horizons - Timmy &amp; Tommy - Nintendo Switch Lite Skin is $4.16 on Amazon https://t.co/JFLW9YusCw #ad https…'),\n",
       " Status(ID=1388644383361220610, ScreenName=Orgetorix, Created=Sat May 01 23:59:55 +0000 2021, Text='Check out this book: \"A Very Dangerous Woman: The Lives, Loves and Lies of Russia\\'s Most Seductive Spy\" by Deborah… https://t.co/5yPbXKxTcX'),\n",
       " Status(ID=1388644383101161478, ScreenName=LaShaWright1, Created=Sat May 01 23:59:55 +0000 2021, Text='@EloualiSabrine Infinite Wisdom \\nhttps://t.co/adBjjZivsw\\nInfinite Wisdom  II\\nhttps://t.co/LqJt0RmXZe\\nThe Fight With… https://t.co/IbeTCCfaKp')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of using twitter.Api to search - using normal parameters\n",
    "api.GetSearch(\n",
    "              term='AMZN', \n",
    "              raw_query=None, \n",
    "              geocode=None, \n",
    "              since_id=None, \n",
    "              max_id=None, \n",
    "              until='2021-05-02', \n",
    "              since=None, \n",
    "              count=3, \n",
    "              lang='en', \n",
    "              locale=None, \n",
    "              result_type='recent', \n",
    "              include_entities=None, \n",
    "              return_json=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using only raw_query to search:** \n",
    "\n",
    "https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets\n",
    "\n",
    "https://twitter.com/search-advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Status(ID=1388644383797485578, ScreenName=thundervolt888, Created=Sat May 01 23:59:55 +0000 2021, Text='RT @Wario64: Animal Crossing: New Horizons - Timmy &amp; Tommy - Nintendo Switch Lite Skin is $4.16 on Amazon https://t.co/JFLW9YusCw #ad https…'),\n",
       " Status(ID=1388644383361220610, ScreenName=Orgetorix, Created=Sat May 01 23:59:55 +0000 2021, Text='Check out this book: \"A Very Dangerous Woman: The Lives, Loves and Lies of Russia\\'s Most Seductive Spy\" by Deborah… https://t.co/5yPbXKxTcX'),\n",
       " Status(ID=1388644383101161478, ScreenName=LaShaWright1, Created=Sat May 01 23:59:55 +0000 2021, Text='@EloualiSabrine Infinite Wisdom \\nhttps://t.co/adBjjZivsw\\nInfinite Wisdom  II\\nhttps://t.co/LqJt0RmXZe\\nThe Fight With… https://t.co/IbeTCCfaKp')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of a search using just raw_query (everything after the “?” in the URL):\n",
    "api.GetSearch(raw_query='q=AMZN%20&until=2021-05-02&lang=en&count=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write tweets to csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*With these functions, specify (term,num_loads) to let it run overtime, or (term,until) to search the last 100 tweets of that day*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_df(term=None, num_loads=1, until=None, lang=None,geocode=None):\n",
    "    import time\n",
    "    df = pandas.DataFrame(columns=['ID','Created','Text'] )\n",
    "\n",
    "    for i in range(num_loads):\n",
    "        print(\"loading data round=\", i+1)\n",
    "        results = api.GetSearch(\n",
    "                                term=term,\n",
    "                                until=until,\n",
    "                                count=100,\n",
    "                                lang=lang,\n",
    "                                geocode=geocode\n",
    "                                )\n",
    "        new_df = pandas.DataFrame({'ID':[results[i].id for i in range(len(results))],\n",
    "                                'Created':[results[i].created_at for i in range(len(results))],\n",
    "                                'Text':[results[i].text for i in range(len(results))]})\n",
    "        df = df.append(new_df, ignore_index=True)\n",
    "        print(\"Total records = \", len(df))\n",
    "        time.sleep(10)\n",
    "    return df\n",
    "\n",
    "def tweets_to_csv(term=None, num_loads=1, until=None, lang=None,geocode=None):\n",
    "    import datetime\n",
    "    \n",
    "    fileName='1.pulledTweets-'+term+'-created at '+str(datetime.datetime.now())+'.csv'\n",
    "    df=tweets_to_df(term=term, num_loads=num_loads, until=until, lang=lang,geocode=geocode)\n",
    "    df.to_csv('../data/'+fileName)\n",
    "    print('Saved in \"'+fileName+'\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For now I dont see the different betweet each load, maybe the time.sleep is too short*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data round= 1\n",
      "Total records =  100\n",
      "loading data round= 2\n",
      "Total records =  200\n",
      "Saved in \"1.pulledTweets-AMZN-created at 2021-05-04 02:33:06.073846.csv\"\n"
     ]
    }
   ],
   "source": [
    "tweets_to_csv(term='AMZN',num_loads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write tweets to json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_jsonString(term=None, num_loads=1, until=None, lang=None,geocode=None):\n",
    "    import json\n",
    "\n",
    "    jsonString=''\n",
    "    \n",
    "    for i in range(num_loads):\n",
    "        print(\"loading data round=\", i+1)\n",
    "        results = api.GetSearch(\n",
    "                                term=term,\n",
    "                                until=until,\n",
    "                                count=100,\n",
    "                                lang=lang,\n",
    "                                geocode=geocode,\n",
    "                                return_json=True\n",
    "                                )\n",
    "        \n",
    "        items=results.get('statuses')\n",
    "        for i in items:\n",
    "            jsonLine = json.dumps(i)+'\\n'\n",
    "            jsonString+=jsonLine\n",
    "        time.sleep(10)\n",
    "        \n",
    "    return jsonString\n",
    "\n",
    "def tweets_to_json(term=None, num_loads=1, until=None, lang=None,geocode=None):\n",
    "    import datetime\n",
    "\n",
    "    fileName='1.pulledTweets-'+term+'-created at '+str(datetime.datetime.now())+'.json'\n",
    "    jsonFile = open('../data/'+fileName, \"w\")\n",
    "    jsonString= tweets_to_jsonString(term=term, num_loads=num_loads, until=until, lang=lang,geocode=geocode)\n",
    "    jsonFile.write(jsonString)\n",
    "    jsonFile.close()\n",
    "    print('Saved in \"'+fileName+'\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data round= 1\n",
      "loading data round= 2\n",
      "Saved in \"1.pulledTweets-AMZN-created at 2021-05-04 02:21:00.052347.json\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_to_json(term='AMZN',num_loads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data round= 1\n",
      "loading data round= 1\n",
      "loading data round= 1\n",
      "loading data round= 1\n",
      "loading data round= 1\n",
      "loading data round= 1\n",
      "loading data round= 1\n"
     ]
    }
   ],
   "source": [
    "# Getting draft data for later notebooks, pulling 7 days x 100 tweets, json\n",
    "day1= tweets_to_jsonString(term='AMZN', until='2021-05-04')\n",
    "day2= tweets_to_jsonString(term='AMZN', until='2021-05-03')\n",
    "day3= tweets_to_jsonString(term='AMZN', until='2021-05-02')\n",
    "day4= tweets_to_jsonString(term='AMZN', until='2021-05-01')\n",
    "day5= tweets_to_jsonString(term='AMZN', until='2021-04-30')\n",
    "day6= tweets_to_jsonString(term='AMZN', until='2021-04-29')\n",
    "day7= tweets_to_jsonString(term='AMZN', until='2021-04-28')\n",
    "jsonString= day1+day2+day3+day4+day5+day6+day7\n",
    "\n",
    "jsonFile = open('../data/1.pulledTweets.json', \"w\")\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
