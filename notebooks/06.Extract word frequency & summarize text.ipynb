{
 "cells": [
  {
   "source": [
    "# Word Frequency\n",
    "- input: \n",
    "    - data4: dataframe with 'text' columns\n",
    "- output: \n",
    "    - data4: added 'text_clearn' columns which is spacy tokens of 'text' columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  index                      created_at  \\\n",
       "219         219           227  251.0  Tue Apr 27 23:58:10 +0000 2021   \n",
       "220         220           228  252.0  Tue Apr 27 23:58:10 +0000 2021   \n",
       "221         221           229  253.0  Tue Apr 27 23:58:08 +0000 2021   \n",
       "222         222           230  254.0  Tue Apr 27 23:58:04 +0000 2021   \n",
       "223         223           231  255.0  Tue Apr 27 23:58:04 +0000 2021   \n",
       "\n",
       "               id                                               text language  \\\n",
       "219  1.387194e+18  Sometimes you get a third chance   Love s Thir...       en   \n",
       "220  1.387194e+18  Check out this Amazon deal  The Art of My Neig...       en   \n",
       "221  1.387194e+18    First Steps  How Upright Walking Made Us Hum...       en   \n",
       "222  1.387194e+18    The First Day of Spring by Nancy Tucker    B...       en   \n",
       "223  1.387194e+18  ad              off     Galaxy Star Projector ...       en   \n",
       "\n",
       "                                            shingleSet  \\\n",
       "219  {1975646848, 3508350977, 2277555713, 187718236...   \n",
       "220  {622155522, 3082850439, 1706091273, 1593029644...   \n",
       "221  {2352757504, 1318699267, 463207690, 4013102741...   \n",
       "222  {3712246153, 2391777162, 2684049684, 401310274...   \n",
       "223  {2473481, 4013102741, 566037670, 2455870758, 1...   \n",
       "\n",
       "                                             signature  \n",
       "219  [18622085, 32776440, 21561465, 191860621, 9160...  \n",
       "220  [116119196, 134412392, 15045377, 922378, 36390...  \n",
       "221  [77244016, 247586965, 343106086, 653196371, 20...  \n",
       "222  [8379671, 18718807, 281343041, 177753045, 3669...  \n",
       "223  [144950540, 122747773, 41607353, 97773221, 829...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 0.1</th>\n      <th>index</th>\n      <th>created_at</th>\n      <th>id</th>\n      <th>text</th>\n      <th>language</th>\n      <th>shingleSet</th>\n      <th>signature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>219</th>\n      <td>219</td>\n      <td>227</td>\n      <td>251.0</td>\n      <td>Tue Apr 27 23:58:10 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>Sometimes you get a third chance   Love s Thir...</td>\n      <td>en</td>\n      <td>{1975646848, 3508350977, 2277555713, 187718236...</td>\n      <td>[18622085, 32776440, 21561465, 191860621, 9160...</td>\n    </tr>\n    <tr>\n      <th>220</th>\n      <td>220</td>\n      <td>228</td>\n      <td>252.0</td>\n      <td>Tue Apr 27 23:58:10 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>Check out this Amazon deal  The Art of My Neig...</td>\n      <td>en</td>\n      <td>{622155522, 3082850439, 1706091273, 1593029644...</td>\n      <td>[116119196, 134412392, 15045377, 922378, 36390...</td>\n    </tr>\n    <tr>\n      <th>221</th>\n      <td>221</td>\n      <td>229</td>\n      <td>253.0</td>\n      <td>Tue Apr 27 23:58:08 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>First Steps  How Upright Walking Made Us Hum...</td>\n      <td>en</td>\n      <td>{2352757504, 1318699267, 463207690, 4013102741...</td>\n      <td>[77244016, 247586965, 343106086, 653196371, 20...</td>\n    </tr>\n    <tr>\n      <th>222</th>\n      <td>222</td>\n      <td>230</td>\n      <td>254.0</td>\n      <td>Tue Apr 27 23:58:04 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>The First Day of Spring by Nancy Tucker    B...</td>\n      <td>en</td>\n      <td>{3712246153, 2391777162, 2684049684, 401310274...</td>\n      <td>[8379671, 18718807, 281343041, 177753045, 3669...</td>\n    </tr>\n    <tr>\n      <th>223</th>\n      <td>223</td>\n      <td>231</td>\n      <td>255.0</td>\n      <td>Tue Apr 27 23:58:04 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>ad              off     Galaxy Star Projector ...</td>\n      <td>en</td>\n      <td>{2473481, 4013102741, 566037670, 2455870758, 1...</td>\n      <td>[144950540, 122747773, 41607353, 97773221, 829...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas\n",
    "data4=pandas.read_csv('../data/5.pulledTweet-deduplicated.csv')\n",
    "data4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  index                      created_at  \\\n",
       "219         219           227  251.0  Tue Apr 27 23:58:10 +0000 2021   \n",
       "220         220           228  252.0  Tue Apr 27 23:58:10 +0000 2021   \n",
       "221         221           229  253.0  Tue Apr 27 23:58:08 +0000 2021   \n",
       "222         222           230  254.0  Tue Apr 27 23:58:04 +0000 2021   \n",
       "223         223           231  255.0  Tue Apr 27 23:58:04 +0000 2021   \n",
       "\n",
       "               id                                               text language  \\\n",
       "219  1.387194e+18  Sometimes you get a third chance   Love s Thir...       en   \n",
       "220  1.387194e+18  Check out this Amazon deal  The Art of My Neig...       en   \n",
       "221  1.387194e+18    First Steps  How Upright Walking Made Us Hum...       en   \n",
       "222  1.387194e+18    The First Day of Spring by Nancy Tucker    B...       en   \n",
       "223  1.387194e+18  ad              off     Galaxy Star Projector ...       en   \n",
       "\n",
       "                                            shingleSet  \\\n",
       "219  {1975646848, 3508350977, 2277555713, 187718236...   \n",
       "220  {622155522, 3082850439, 1706091273, 1593029644...   \n",
       "221  {2352757504, 1318699267, 463207690, 4013102741...   \n",
       "222  {3712246153, 2391777162, 2684049684, 401310274...   \n",
       "223  {2473481, 4013102741, 566037670, 2455870758, 1...   \n",
       "\n",
       "                                             signature  \\\n",
       "219  [18622085, 32776440, 21561465, 191860621, 9160...   \n",
       "220  [116119196, 134412392, 15045377, 922378, 36390...   \n",
       "221  [77244016, 247586965, 343106086, 653196371, 20...   \n",
       "222  [8379671, 18718807, 281343041, 177753045, 3669...   \n",
       "223  [144950540, 122747773, 41607353, 97773221, 829...   \n",
       "\n",
       "                                                 token  \\\n",
       "219  [sometimes, you, get, third, chance,   , love,...   \n",
       "220  [check, out, this, amazon, deal, the, art, of,...   \n",
       "221  [  , first, step, how, upright, walking, make,...   \n",
       "222  [  , the, first, day, of, spring, by, nancy, t...   \n",
       "223  [             , off,     , galaxy, star, proje...   \n",
       "\n",
       "                                            text_clean  \n",
       "219  sometimes you get third chance    love third c...  \n",
       "220  check out this amazon deal the art of my neigh...  \n",
       "221     first step how upright walking make we huma...  \n",
       "222     the first day of spring by nancy tucker    ...  \n",
       "223                off      galaxy star projector w...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 0.1</th>\n      <th>index</th>\n      <th>created_at</th>\n      <th>id</th>\n      <th>text</th>\n      <th>language</th>\n      <th>shingleSet</th>\n      <th>signature</th>\n      <th>token</th>\n      <th>text_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>219</th>\n      <td>219</td>\n      <td>227</td>\n      <td>251.0</td>\n      <td>Tue Apr 27 23:58:10 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>Sometimes you get a third chance   Love s Thir...</td>\n      <td>en</td>\n      <td>{1975646848, 3508350977, 2277555713, 187718236...</td>\n      <td>[18622085, 32776440, 21561465, 191860621, 9160...</td>\n      <td>[sometimes, you, get, third, chance,   , love,...</td>\n      <td>sometimes you get third chance    love third c...</td>\n    </tr>\n    <tr>\n      <th>220</th>\n      <td>220</td>\n      <td>228</td>\n      <td>252.0</td>\n      <td>Tue Apr 27 23:58:10 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>Check out this Amazon deal  The Art of My Neig...</td>\n      <td>en</td>\n      <td>{622155522, 3082850439, 1706091273, 1593029644...</td>\n      <td>[116119196, 134412392, 15045377, 922378, 36390...</td>\n      <td>[check, out, this, amazon, deal, the, art, of,...</td>\n      <td>check out this amazon deal the art of my neigh...</td>\n    </tr>\n    <tr>\n      <th>221</th>\n      <td>221</td>\n      <td>229</td>\n      <td>253.0</td>\n      <td>Tue Apr 27 23:58:08 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>First Steps  How Upright Walking Made Us Hum...</td>\n      <td>en</td>\n      <td>{2352757504, 1318699267, 463207690, 4013102741...</td>\n      <td>[77244016, 247586965, 343106086, 653196371, 20...</td>\n      <td>[  , first, step, how, upright, walking, make,...</td>\n      <td>first step how upright walking make we huma...</td>\n    </tr>\n    <tr>\n      <th>222</th>\n      <td>222</td>\n      <td>230</td>\n      <td>254.0</td>\n      <td>Tue Apr 27 23:58:04 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>The First Day of Spring by Nancy Tucker    B...</td>\n      <td>en</td>\n      <td>{3712246153, 2391777162, 2684049684, 401310274...</td>\n      <td>[8379671, 18718807, 281343041, 177753045, 3669...</td>\n      <td>[  , the, first, day, of, spring, by, nancy, t...</td>\n      <td>the first day of spring by nancy tucker    ...</td>\n    </tr>\n    <tr>\n      <th>223</th>\n      <td>223</td>\n      <td>231</td>\n      <td>255.0</td>\n      <td>Tue Apr 27 23:58:04 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>ad              off     Galaxy Star Projector ...</td>\n      <td>en</td>\n      <td>{2473481, 4013102741, 566037670, 2455870758, 1...</td>\n      <td>[144950540, 122747773, 41607353, 97773221, 829...</td>\n      <td>[             , off,     , galaxy, star, proje...</td>\n      <td>off      galaxy star projector w...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Generate clean tokens for all documents\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) #just keep tagger for lemmatization, disable 'parser' and 'ner'\n",
    "\n",
    "def token_filter(token):\n",
    "    return not (token.is_punct | token.is_space | token.is_stop | len(token.text) <= 2)\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    for doc in nlp.pipe([str(text).lower()]):\n",
    "        tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
    "    return tokens\n",
    "\n",
    "data4['token'] = data4['text'].map(spacy_tokenizer) \n",
    "data4['text_clean'] = data4.apply(lambda x: \" \".join(x['token']), axis=1)\n",
    "\n",
    "data4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                 bigram  count\n",
       "0       infinite wisdom infinite wisdom      9\n",
       "1          wisdom infinite wisdom fight      9\n",
       "2          infinite wisdom fight within      6\n",
       "3               truth win suffice fight      5\n",
       "4                win suffice fight meet      5\n",
       "5  suffice fight meet vakeelsaabonprime      5\n",
       "6                ring light bundle save      4\n",
       "7               light bundle save promo      4\n",
       "8                bundle save promo code      4\n",
       "9                  save promo code rfdk      4"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bigram</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>infinite wisdom infinite wisdom</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wisdom infinite wisdom fight</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>infinite wisdom fight within</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>truth win suffice fight</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>win suffice fight meet</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>suffice fight meet vakeelsaabonprime</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ring light bundle save</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>light bundle save promo</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>bundle save promo code</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>save promo code rfdk</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def get_top_phrase(corpus, numPhrases=None, numWords=1):\n",
    "    \n",
    "    vec = CountVectorizer(ngram_range=(numWords, numWords), stop_words=nltk.corpus.stopwords.words('english')).fit(corpus.astype(str))\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:numPhrases]\n",
    "\n",
    "common_words = get_top_phrase(data4['text_clean'], numPhrases=10,numWords=4)\n",
    "pandas.DataFrame(common_words, columns = ['bigram' , 'count'])"
   ]
  },
  {
   "source": [
    "# Text summarization using word frequency (extractive approach)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all articles into a big string\n",
    "big_doc=''\n",
    "for i in range(len(data4)):\n",
    "    big_doc+=str(data4.text[i])+'.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\nWall time: 8.82 µs\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'pack of LED RGB Flood Lights            Coupon PLUS Save     with promo code   TKHBLS    .What makes a place   Haunted   Award winning book by Kitty Janusz  Now on Amazon  amp  Kindle  .\\nAnimal Crossing  New Horizons   Timmy  amp  Tommy   Nintendo Switch Lite Skin is       on Amazon   ad https .Check out this book   A Very Dangerous Woman  The Lives  Loves and Lies of Russia s Most Seductive Spy  by Deborah  .\\nThe truth won t suffice  he will have to fight for it    Meet  VakeelSaabOnPrime     .Grab a   pack of LED Ring Lights  get it for         Save     with promo code   FUL     .Winning After Losing  Building Resilient Teams is an important book  Learn more    .Kenwood FP    Compact Food Processor   Silver And Grey        delivered   Amazon  a     Foodie  Foodies  food  .'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# OPTION 1 #very slow for alot of big documents\n",
    "from gensim.summarization import summarize \n",
    "%time\n",
    "summarize(big_doc, ratio = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 5 µs, sys: 5 µs, total: 10 µs\n",
      "Wall time: 23.1 µs\n",
      "Average sentence scores is:  3.2945205479452055 . Triple it to use as threahold for summary funciton below?\n"
     ]
    }
   ],
   "source": [
    "# OPTION 2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "%time\n",
    "\n",
    "sentences = sent_tokenize(big_doc)\n",
    "sentences = [sentence for sentence in sentences if len(sentence) >= 30]\n",
    "sentences = list(set(sentences))\n",
    "freq_table = dict(get_top_phrase(data4['text'].astype(str),numWords=1))\n",
    "\n",
    "def score_sentences(sentences, freq_table):\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        word_count_in_sentence = len(word_tokenize(sentence))\n",
    "        for word in freq_table:\n",
    "            if word.lower() in sentence.lower():                \n",
    "                if sentence in sentence_scores:\n",
    "                    sentence_scores[sentence] += freq_table[word]\n",
    "                else:\n",
    "                    sentence_scores[sentence] = freq_table[word]\n",
    "        sentence_scores[sentence] = sentence_scores[sentence] // word_count_in_sentence\n",
    "    return sentence_scores\n",
    "sentence_scores = score_sentences(sentences, freq_table)\n",
    "\n",
    "\n",
    "def average_sentence_scores(sentence_scores):\n",
    "    sum=0\n",
    "    for key, value in sentence_scores.items():\n",
    "        sum+=value\n",
    "    return sum/len(sentence_scores)\n",
    "print('Average sentence scores is: ',average_sentence_scores(sentence_scores),'. Triple it to use as threahold for summary funciton below?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' Infinite Wisdom   Infinite Wisdom  II  The Fight Within . Shonen Jump  Undead Unluck Volume   pre orders are up at Rightstuf  Amazon  B amp N . Q  Revenue Growth  YoY   Change    Tesla  TSLA         Apple  AAPL         Facebook  FB         Amazon  AMZN         Goo . King  amp  Prince  CM                    Amazon         net  . Infinite Wisdom   Infinite Wisdom  II  The Fight Wit  . Ring Light Bundle for           Save     with promo code   RFDK           Seller MUST be  . Sonos Playbar Black Works with PS   amp  PS  via TV            Amazon USA  . Reader Ready Award Recommended Read        asmsg  iartg  amreading  bookboost https    . Monster Hunter Chibi Plush pre orders  Amazon    ad        Play Asia          .STEAL      pack of LED Video Photo Lighting Kit            Save     with promo code   FDUL     .'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "def generate_summary(sentences, sentence_scores, threshold): # Use average_sentence_scores as clue for threshold\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "    for sentence in sentences:\n",
    "        if sentence in sentence_scores and sentence_scores[sentence] > threshold:\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "    return summary \n",
    "\n",
    "generate_summary(sentences, sentence_scores, 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python370jvsc74a57bd0398dc28c06ad810e77de546bbdfa897a6ee0b83e59a5207339dda01a7843e01d",
   "display_name": "Python 3.7.0 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}