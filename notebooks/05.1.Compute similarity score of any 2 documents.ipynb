{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Build function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_tag(tag):\n",
    "    \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "   \n",
    "    tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "    try:\n",
    "        return tag_dict[tag[0]]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def doc_to_synsets(doc:str):\n",
    "    \"\"\"\n",
    "    Returns a list of synsets in document.\n",
    "    Example:\n",
    "        doc_to_synsets('Fish are nvqjp friends.')\n",
    "        Out: [Synset('fish.n.01'), Synset('be.v.01'), Synset('friend.n.01')]\n",
    "    \"\"\"\n",
    "    \n",
    "    wordTokens=nltk.word_tokenize(doc)\n",
    "    wordPOStags=nltk.pos_tag(wordTokens)\n",
    "    temp=[wn.synsets(token, convert_tag(wordPOStag)) for token,wordPOStag in wordPOStags]\n",
    "    result=[]\n",
    "    for i in range (len(temp)):\n",
    "        if temp[i]:\n",
    "            result+=[temp[i][0]]\n",
    "    return result\n",
    "\n",
    "\n",
    "def similarity_score(s1, s2):\n",
    "    \"\"\"\n",
    "    Calculate the normalized similarity score of s1 onto s2\n",
    "\n",
    "    Example:\n",
    "        synsets1 = doc_to_synsets('I like cats')\n",
    "        synsets2 = doc_to_synsets('I like dogs')\n",
    "        similarity_score(synsets1, synsets2)\n",
    "        Out: 0.73333333333333339\n",
    "    \"\"\"  \n",
    "    s=[]\n",
    "    for i in s1:\n",
    "        scores=[x for x in [i.path_similarity(j) for j in s2]if x is not None]\n",
    "        if scores:\n",
    "            s.append(max(scores))\n",
    "    \n",
    "    return sum(s)/len(s)\n",
    "\n",
    "\n",
    "def document_path_similarity(doc1, doc2):\n",
    "    \"\"\"Finds the symmetrical similarity between doc1 and doc2\"\"\"\n",
    "\n",
    "    synsets1 = doc_to_synsets(doc1)\n",
    "    synsets2 = doc_to_synsets(doc2)\n",
    "\n",
    "    return (similarity_score(synsets1, synsets2) + similarity_score(synsets2, synsets1)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The similarity of all messages and \"you should buy Amazon stock\" is: 0.5724405856426191\n"
     ]
    }
   ],
   "source": [
    "txt=''\n",
    "for message in pd.read_csv('../data/4.Tweet data after clear spams.csv').text: txt+=str(message)\n",
    "message_to_compare='you should buy Amazon stock'\n",
    "print('The similarity of all messages and \"'+message_to_compare+'\" is:', document_path_similarity(txt,message_to_compare))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python370jvsc74a57bd0398dc28c06ad810e77de546bbdfa897a6ee0b83e59a5207339dda01a7843e01d",
   "display_name": "Python 3.7.0 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}