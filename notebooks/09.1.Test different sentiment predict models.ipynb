{
 "cells": [
  {
   "source": [
    "**Training datasets**\n",
    "\n",
    "**'9.Sentiment Training 1.csv'** was downloaded at https://www.kaggle.com/kazanova/sentiment140. It contains the following 6 fields:\n",
    "- target: the polarity of the tweet (0 = negative, 4 = positive)\n",
    "- ids: The id of the tweet ( 2087)\n",
    "- date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "- flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "- user: the user that tweeted (robotickilldozr)\n",
    "- text: the text of the tweet (Lyx is cool)\n",
    "\n",
    "This file is over 200MB\n",
    "\n",
    "**'9.Sentiment Training 2.csv'** is another Tweets sentiment dataset gotten from https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "This file is only 4MB"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Testing Unsupervised Nltk Vader sentiment score:\n",
    "\n",
    "http://www.nltk.org/howto/sentiment.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***Using the large training file***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.662510525914678"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "def vader_sentiment_score():\n",
    "    import pandas as pd\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#     traning data\n",
    "    sentiment_data_df = pd.read_csv('../data/9.Sentiment Training 1.csv',encoding='latin-1', names=['target','ids','date','flag','user','text'])\n",
    "    sentiment_data_df = sentiment_data_df.sample(frac=0.1, random_state=0)\n",
    "    sentiment_data_df['target']=sentiment_data_df['target'].replace(4, 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sentiment_data_df['text'], sentiment_data_df['target'], random_state=0)\n",
    "    \n",
    "#     vader model\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "  \n",
    "#     evaluate vader model\n",
    "    vader_predictions = [sid.polarity_scores(s)['compound'] for s in X_test]\n",
    "    for i in range (len(vader_predictions)):\n",
    "        if vader_predictions[i]<0: \n",
    "            vader_predictions[i]=0\n",
    "        else:vader_predictions[i]=1\n",
    "    return roc_auc_score(y_test, vader_predictions)\n",
    "vader_sentiment_score()"
   ]
  },
  {
   "source": [
    "***Using the small training file***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5465163934426229"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# import data from file\n",
    "import pandas as pd\n",
    "df=pd.read_csv('../data/9.Sentiment Training 2.csv', usecols=['text','airline_sentiment'])\n",
    "\n",
    "# get sentiment using nltk vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "df[\"vader_sentiment\"] =''\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for i in range(0,len(df)):\n",
    "    ss = sid.polarity_scores(df.text[i])\n",
    "    if ss['compound']>0:\n",
    "        df.vader_sentiment[i]=\"positive\"\n",
    "    elif ss['compound']<0:\n",
    "        df.vader_sentiment[i]=\"negative\"\n",
    "    else:\n",
    "        df.vader_sentiment[i]=\"neutral\"\n",
    "\n",
    "# Prediction Accuracy Score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(df.airline_sentiment,df.vader_sentiment)"
   ]
  },
  {
   "source": [
    "*Does not look like NLTK Vader has performed very good on both files*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Supervised Sentiment Predict Models"
   ]
  },
  {
   "source": [
    "***Using large traing file***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "rn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           Classifier  TfidfVectorizer_ngram  \\\n",
       "0   MultinomialNB(alpha=1.0, class_prior=None, fit...                      1   \n",
       "1   BernoulliNB(alpha=1.0, binarize=0.0, class_pri...                      1   \n",
       "2   LogisticRegression(C=1.0, class_weight=None, d...                      1   \n",
       "3   LinearSVC(C=1.0, class_weight=None, dual=True,...                      1   \n",
       "4   (DecisionTreeClassifier(class_weight=None, cri...                      1   \n",
       "5   RidgeClassifier(alpha=1.0, class_weight=None, ...                      1   \n",
       "6   PassiveAggressiveClassifier(C=1.0, average=Fal...                      1   \n",
       "7   Perceptron(alpha=0.0001, class_weight=None, et...                      1   \n",
       "8   MultinomialNB(alpha=1.0, class_prior=None, fit...                      2   \n",
       "9   BernoulliNB(alpha=1.0, binarize=0.0, class_pri...                      2   \n",
       "10  LogisticRegression(C=1.0, class_weight=None, d...                      2   \n",
       "11  LinearSVC(C=1.0, class_weight=None, dual=True,...                      2   \n",
       "12  (DecisionTreeClassifier(class_weight=None, cri...                      2   \n",
       "13  RidgeClassifier(alpha=1.0, class_weight=None, ...                      2   \n",
       "14  PassiveAggressiveClassifier(C=1.0, average=Fal...                      2   \n",
       "15  Perceptron(alpha=0.0001, class_weight=None, et...                      2   \n",
       "16  MultinomialNB(alpha=1.0, class_prior=None, fit...                      1   \n",
       "17  BernoulliNB(alpha=1.0, binarize=0.0, class_pri...                      1   \n",
       "18  LogisticRegression(C=1.0, class_weight=None, d...                      1   \n",
       "19  LinearSVC(C=1.0, class_weight=None, dual=True,...                      1   \n",
       "20  (DecisionTreeClassifier(class_weight=None, cri...                      1   \n",
       "21  RidgeClassifier(alpha=1.0, class_weight=None, ...                      1   \n",
       "22  PassiveAggressiveClassifier(C=1.0, average=Fal...                      1   \n",
       "23  Perceptron(alpha=0.0001, class_weight=None, et...                      1   \n",
       "24  MultinomialNB(alpha=1.0, class_prior=None, fit...                      2   \n",
       "25  BernoulliNB(alpha=1.0, binarize=0.0, class_pri...                      2   \n",
       "26  LogisticRegression(C=1.0, class_weight=None, d...                      2   \n",
       "27  LinearSVC(C=1.0, class_weight=None, dual=True,...                      2   \n",
       "28  (DecisionTreeClassifier(class_weight=None, cri...                      2   \n",
       "29  RidgeClassifier(alpha=1.0, class_weight=None, ...                      2   \n",
       "30  PassiveAggressiveClassifier(C=1.0, average=Fal...                      2   \n",
       "31  Perceptron(alpha=0.0001, class_weight=None, et...                      2   \n",
       "\n",
       "    TfidfVectorizer_mindf       Ac  crossval_train_score_mean  \\\n",
       "0                       1  0.73650                   0.929917   \n",
       "1                       1  0.74225                   0.927000   \n",
       "2                       1  0.75250                   0.878917   \n",
       "3                       1  0.74650                   0.985917   \n",
       "4                       1  0.67475                   0.698333   \n",
       "5                       1  0.74625                   0.963333   \n",
       "6                       1  0.73600                   0.992083   \n",
       "7                       1  0.70150                   0.967083   \n",
       "8                       1  0.75825                   0.988750   \n",
       "9                       1  0.73275                   0.984167   \n",
       "10                      1  0.74850                   0.940750   \n",
       "11                      1  0.75800                   0.999667   \n",
       "12                      1  0.68550                   0.695417   \n",
       "13                      1  0.75750                   0.997667   \n",
       "14                      1  0.75575                   0.999833   \n",
       "15                      1  0.73925                   0.996000   \n",
       "16                      2  0.74375                   0.859583   \n",
       "17                      2  0.74525                   0.847833   \n",
       "18                      2  0.75275                   0.844417   \n",
       "19                      2  0.73825                   0.918417   \n",
       "20                      2  0.67850                   0.694667   \n",
       "21                      2  0.74125                   0.900000   \n",
       "22                      2  0.72825                   0.920667   \n",
       "23                      2  0.70075                   0.862833   \n",
       "24                      2  0.75000                   0.906667   \n",
       "25                      2  0.73700                   0.891250   \n",
       "26                      2  0.75625                   0.882083   \n",
       "27                      2  0.74475                   0.982167   \n",
       "28                      2  0.67900                   0.692667   \n",
       "29                      2  0.74775                   0.960250   \n",
       "30                      2  0.73275                   0.990250   \n",
       "31                      2  0.70800                   0.968500   \n",
       "\n",
       "    crossval_test_score_mean      Time  Ac_rank  Time_rank  \n",
       "0                   0.723500  0.987393     21.0       28.0  \n",
       "1                   0.727167  1.035883     16.0       22.0  \n",
       "2                   0.742000  1.026617      7.0       24.0  \n",
       "3                   0.731750  1.028660     11.0       23.0  \n",
       "4                   0.670417  3.804790     32.0        3.0  \n",
       "5                   0.736667  1.173594     12.0       19.0  \n",
       "6                   0.717333  1.022122     22.0       25.0  \n",
       "7                   0.700250  1.000475     27.0       27.0  \n",
       "8                   0.736250  2.417460      1.0       13.0  \n",
       "9                   0.715417  2.355972     23.5       14.0  \n",
       "10                  0.741417  2.589673      9.0       10.0  \n",
       "11                  0.742750  2.996860      2.0        9.0  \n",
       "12                  0.674917  7.694590     29.0        1.0  \n",
       "13                  0.744917  3.113187      3.0        6.0  \n",
       "14                  0.738000  3.310421      5.0        5.0  \n",
       "15                  0.715417  3.060995     18.0        8.0  \n",
       "16                  0.726417  1.019280     15.0       26.0  \n",
       "17                  0.725917  0.960543     13.0       30.0  \n",
       "18                  0.743667  0.986404      6.0       29.0  \n",
       "19                  0.723833  1.069916     19.0       21.0  \n",
       "20                  0.665500  3.069579     31.0        7.0  \n",
       "21                  0.730833  1.128965     17.0       20.0  \n",
       "22                  0.714167  0.956426     25.0       31.0  \n",
       "23                  0.685583  0.945461     28.0       32.0  \n",
       "24                  0.735750  2.325058      8.0       17.0  \n",
       "25                  0.721250  2.273397     20.0       18.0  \n",
       "26                  0.746167  2.441031      4.0       11.0  \n",
       "27                  0.725667  2.337814     14.0       15.0  \n",
       "28                  0.664917  5.685250     30.0        2.0  \n",
       "29                  0.732750  3.706983     10.0        4.0  \n",
       "30                  0.709500  2.330473     23.5       16.0  \n",
       "31                  0.693167  2.418135     26.0       12.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Classifier</th>\n      <th>TfidfVectorizer_ngram</th>\n      <th>TfidfVectorizer_mindf</th>\n      <th>Ac</th>\n      <th>crossval_train_score_mean</th>\n      <th>crossval_test_score_mean</th>\n      <th>Time</th>\n      <th>Ac_rank</th>\n      <th>Time_rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.73650</td>\n      <td>0.929917</td>\n      <td>0.723500</td>\n      <td>0.987393</td>\n      <td>21.0</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BernoulliNB(alpha=1.0, binarize=0.0, class_pri...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.74225</td>\n      <td>0.927000</td>\n      <td>0.727167</td>\n      <td>1.035883</td>\n      <td>16.0</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.75250</td>\n      <td>0.878917</td>\n      <td>0.742000</td>\n      <td>1.026617</td>\n      <td>7.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.74650</td>\n      <td>0.985917</td>\n      <td>0.731750</td>\n      <td>1.028660</td>\n      <td>11.0</td>\n      <td>23.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.67475</td>\n      <td>0.698333</td>\n      <td>0.670417</td>\n      <td>3.804790</td>\n      <td>32.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>RidgeClassifier(alpha=1.0, class_weight=None, ...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.74625</td>\n      <td>0.963333</td>\n      <td>0.736667</td>\n      <td>1.173594</td>\n      <td>12.0</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>PassiveAggressiveClassifier(C=1.0, average=Fal...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.73600</td>\n      <td>0.992083</td>\n      <td>0.717333</td>\n      <td>1.022122</td>\n      <td>22.0</td>\n      <td>25.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Perceptron(alpha=0.0001, class_weight=None, et...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.70150</td>\n      <td>0.967083</td>\n      <td>0.700250</td>\n      <td>1.000475</td>\n      <td>27.0</td>\n      <td>27.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.75825</td>\n      <td>0.988750</td>\n      <td>0.736250</td>\n      <td>2.417460</td>\n      <td>1.0</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>BernoulliNB(alpha=1.0, binarize=0.0, class_pri...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.73275</td>\n      <td>0.984167</td>\n      <td>0.715417</td>\n      <td>2.355972</td>\n      <td>23.5</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.74850</td>\n      <td>0.940750</td>\n      <td>0.741417</td>\n      <td>2.589673</td>\n      <td>9.0</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.75800</td>\n      <td>0.999667</td>\n      <td>0.742750</td>\n      <td>2.996860</td>\n      <td>2.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.68550</td>\n      <td>0.695417</td>\n      <td>0.674917</td>\n      <td>7.694590</td>\n      <td>29.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>RidgeClassifier(alpha=1.0, class_weight=None, ...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.75750</td>\n      <td>0.997667</td>\n      <td>0.744917</td>\n      <td>3.113187</td>\n      <td>3.0</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>PassiveAggressiveClassifier(C=1.0, average=Fal...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.75575</td>\n      <td>0.999833</td>\n      <td>0.738000</td>\n      <td>3.310421</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Perceptron(alpha=0.0001, class_weight=None, et...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.73925</td>\n      <td>0.996000</td>\n      <td>0.715417</td>\n      <td>3.060995</td>\n      <td>18.0</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.74375</td>\n      <td>0.859583</td>\n      <td>0.726417</td>\n      <td>1.019280</td>\n      <td>15.0</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>BernoulliNB(alpha=1.0, binarize=0.0, class_pri...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.74525</td>\n      <td>0.847833</td>\n      <td>0.725917</td>\n      <td>0.960543</td>\n      <td>13.0</td>\n      <td>30.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.75275</td>\n      <td>0.844417</td>\n      <td>0.743667</td>\n      <td>0.986404</td>\n      <td>6.0</td>\n      <td>29.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.73825</td>\n      <td>0.918417</td>\n      <td>0.723833</td>\n      <td>1.069916</td>\n      <td>19.0</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.67850</td>\n      <td>0.694667</td>\n      <td>0.665500</td>\n      <td>3.069579</td>\n      <td>31.0</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>RidgeClassifier(alpha=1.0, class_weight=None, ...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.74125</td>\n      <td>0.900000</td>\n      <td>0.730833</td>\n      <td>1.128965</td>\n      <td>17.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>PassiveAggressiveClassifier(C=1.0, average=Fal...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.72825</td>\n      <td>0.920667</td>\n      <td>0.714167</td>\n      <td>0.956426</td>\n      <td>25.0</td>\n      <td>31.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Perceptron(alpha=0.0001, class_weight=None, et...</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.70075</td>\n      <td>0.862833</td>\n      <td>0.685583</td>\n      <td>0.945461</td>\n      <td>28.0</td>\n      <td>32.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>MultinomialNB(alpha=1.0, class_prior=None, fit...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.75000</td>\n      <td>0.906667</td>\n      <td>0.735750</td>\n      <td>2.325058</td>\n      <td>8.0</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>BernoulliNB(alpha=1.0, binarize=0.0, class_pri...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.73700</td>\n      <td>0.891250</td>\n      <td>0.721250</td>\n      <td>2.273397</td>\n      <td>20.0</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.75625</td>\n      <td>0.882083</td>\n      <td>0.746167</td>\n      <td>2.441031</td>\n      <td>4.0</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>LinearSVC(C=1.0, class_weight=None, dual=True,...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.74475</td>\n      <td>0.982167</td>\n      <td>0.725667</td>\n      <td>2.337814</td>\n      <td>14.0</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.67900</td>\n      <td>0.692667</td>\n      <td>0.664917</td>\n      <td>5.685250</td>\n      <td>30.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>RidgeClassifier(alpha=1.0, class_weight=None, ...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.74775</td>\n      <td>0.960250</td>\n      <td>0.732750</td>\n      <td>3.706983</td>\n      <td>10.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>PassiveAggressiveClassifier(C=1.0, average=Fal...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.73275</td>\n      <td>0.990250</td>\n      <td>0.709500</td>\n      <td>2.330473</td>\n      <td>23.5</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Perceptron(alpha=0.0001, class_weight=None, et...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.70800</td>\n      <td>0.968500</td>\n      <td>0.693167</td>\n      <td>2.418135</td>\n      <td>26.0</td>\n      <td>12.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "def test_classifiers():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "    from sklearn.linear_model import RidgeClassifier\n",
    "    from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "    from sklearn.linear_model import Perceptron\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "    from sklearn.model_selection import cross_validate,KFold\n",
    "    import datetime\n",
    "    import time\n",
    "    import pandas as pd\n",
    "\n",
    "    classifiers = [MultinomialNB(),BernoulliNB(),LogisticRegression(),LinearSVC(),AdaBoostClassifier(),RidgeClassifier(),PassiveAggressiveClassifier(),Perceptron()]\n",
    "\n",
    "    #traning data\n",
    "    sentiment_data_df = pd.read_csv('../data/9.Sentiment Training.csv',encoding='latin-1', names=['target','ids','date','flag','user','text'])\n",
    "    sentiment_data_df = sentiment_data_df.sample(frac=0.01, random_state=0)\n",
    "    sentiment_data_df['target']=sentiment_data_df['target'].replace(4, 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sentiment_data_df['text'], sentiment_data_df['target'], random_state=0)\n",
    "\n",
    "    data=[]\n",
    "    for mindf in range(1,3): \n",
    "        for ngram in range(1,3):\n",
    "            for clf in classifiers:\n",
    "\n",
    "                before = datetime.datetime.now()\n",
    "                before = before.strftime(\"%H:%M:%S\")\n",
    "                start = time.time()\n",
    "\n",
    "                vect = TfidfVectorizer(min_df=mindf,ngram_range=(1,ngram))\n",
    "                model = make_pipeline(vect,clf)\n",
    "                model.fit(X_train,y_train)\n",
    "                labels = model.predict(X_test)\n",
    "                ac = accuracy_score(y_test,labels)\n",
    "                kfold = KFold(n_splits=2,shuffle=False,random_state=None)\n",
    "                results = cross_validate(model,X_train,y_train,cv=kfold)\n",
    "                crossval_test_score_mean=results['test_score'].mean()\n",
    "                crossval_train_score_mean=results['train_score'].mean()\n",
    "\n",
    "                after = datetime.datetime.now()\n",
    "                after = after.strftime(\"%H:%M:%S\")\n",
    "                end = time.time()\n",
    "                hours = int(after[0:2])-int(before[0:2])\n",
    "                mins = int(after[3:5])-int(before[3:5])\n",
    "                secs = int(after[6:8])-int(before[6:8])\n",
    "                time_taken = str(hours)+\":\"+str(mins)+\":\"+str(secs)\n",
    "\n",
    "                data.append([clf,ngram, mindf, ac,crossval_train_score_mean,crossval_test_score_mean, end-start])\n",
    "\n",
    "    d = pd.DataFrame(data,columns=['Classifier','TfidfVectorizer_ngram','TfidfVectorizer_mindf','Ac','crossval_train_score_mean','crossval_test_score_mean','Time'])\n",
    "    d['Ac_rank'] = d['Ac'].rank(ascending=False)\n",
    "    d['Time_rank'] = d['Time'].rank(ascending=False)\n",
    "    d.to_csv('../data/9.Sentiment predict models scores.csv')\n",
    "    return d\n",
    "test_classifiers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python370jvsc74a57bd0398dc28c06ad810e77de546bbdfa897a6ee0b83e59a5207339dda01a7843e01d",
   "display_name": "Python 3.7.0 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}