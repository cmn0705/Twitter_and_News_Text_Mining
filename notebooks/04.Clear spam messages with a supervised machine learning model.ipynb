{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build supervised spam detector\n",
    "\n",
    "- Dataset to be used for traning: \"Spam Training.csv\"\n",
    "- Fit and transform the training data X_train using a Count Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **character n-grams from n=2 to n=5.**\n",
    "\n",
    "- To tell Count Vectorizer to use character n-grams pass in `analyzer='char_wb'` which creates character n-grams only from text inside word boundaries. This should make the model more robust to spelling mistakes.\n",
    "\n",
    "- Using this document-term matrix and the following additional features:\n",
    "    * the length of document (number of characters)\n",
    "    * number of digits per document\n",
    "    * **number of non-word characters (anything other than a letter, digit or underscore.)**\n",
    "\n",
    "- fit a Logistic Regression model with regularization C=100. Then compute the area under the curve (AUC) score using the transformed test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spam Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test score: 0.9788593110707434\nlist of smallest coef index: ['. ', '..', '? ', ' i', ' y', ' go', ':)', ' h', 'go', ' m']\nlist of largest coef index: ['digit_count', 'ne', 'ia', 'co', 'xt', ' ch', 'mob', ' x', 'ww', 'ar']\n"
     ]
    }
   ],
   "source": [
    "def spam_model():\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import re\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "    # Import and tranform spam dataset\n",
    "    spam_data = pd.read_csv('../data/4.Spam Training.csv')\n",
    "    spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], spam_data['target'], random_state=0)\n",
    "\n",
    "    # vectorize train, test sets\n",
    "    spam_vect=CountVectorizer(min_df=5, analyzer='char_wb', ngram_range=(2,5)).fit(X_train)\n",
    "    X_train_vectorized = spam_vect.transform(X_train)\n",
    "    X_test_vectorized = spam_vect.transform(X_test)\n",
    "\n",
    "    # add features\n",
    "    num_digit_X_train=X_train.apply(lambda x:len(re.findall(r'\\d', x)))\n",
    "    num_non_word_X_train=X_train.apply(lambda x:len(re.findall(r'\\W', x)))\n",
    "    X_train_vectorized_len_digit_nonword=hstack([X_train_vectorized, csr_matrix([X_train.str.len(), \n",
    "                                                                                 num_digit_X_train, \n",
    "                                                                                 num_non_word_X_train]).T], 'csr')\n",
    "\n",
    "    num_digit_X_test=X_test.apply(lambda x:len(re.findall(r'\\d', x)))\n",
    "    num_non_word_X_test=X_test.apply(lambda x:len(re.findall(r'\\W', x)))\n",
    "    X_test_vectorized_len_digit_nonword=hstack([X_test_vectorized,csr_matrix([X_test.str.len(),\n",
    "                                                                            num_digit_X_test,\n",
    "                                                                            num_non_word_X_test]).T],'csr')\n",
    "\n",
    "    # Creat prediction model\n",
    "    spam_model=LogisticRegression(C=100).fit(X_train_vectorized_len_digit_nonword,y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    feature_names = np.array(spam_vect.get_feature_names()+['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "    sorted_coef_index = spam_model.coef_[0].argsort()\n",
    "    smallest = feature_names[sorted_coef_index[:10]]\n",
    "    largest = feature_names[sorted_coef_index[:-11:-1]]\n",
    "    spam_predictions = spam_model.predict(X_test_vectorized_len_digit_nonword)\n",
    "    print('test score:', roc_auc_score(y_test, spam_predictions))\n",
    "    print('list of smallest coef index:', list(smallest))\n",
    "    print('list of largest coef index:', list(largest))\n",
    "    \n",
    "    return spam_vect, spam_model\n",
    "\n",
    "spam_vect, spam_model=spam_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_predict(docs):\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    import re\n",
    "    docs_vectorized = spam_vect.transform(docs)\n",
    "    num_digit=docs.apply(lambda x:len(re.findall(r'\\d', x)))\n",
    "    num_nonword=docs.apply(lambda x:len(re.findall(r'\\W', x)))\n",
    "    docs_vectorized_len_digit_nonword=hstack([docs_vectorized,csr_matrix([docs.str.len(),\n",
    "                                                                    num_digit,\n",
    "                                                                    num_nonword]).T],'csr')\n",
    "    return spam_model.predict(docs_vectorized_len_digit_nonword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note: Need better spam dataset, or try different spam detection model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      \"House of Cards\" has always hit differently ðŸŽ‚ ...\n",
       "1      POWER A Enhanced Wireless Controller for Ninte...\n",
       "2      Select 1st party Switch $39.99 physical/digita...\n",
       "3      Silicone Eye Pad for Oculus Quest 2 Face Cushi...\n",
       "4      Power Strip Surge Protector- 5 Outlets 3 USB P...\n",
       "                             ...                        \n",
       "227    Sometimes you get a third chance - Love's Thir...\n",
       "228    Check out this Amazon deal: The Art of My Neig...\n",
       "229    RT @bigtickHK: First Steps: How Upright Walkin...\n",
       "230    RT @bigtickHK: The First Day of Spring by Nanc...\n",
       "231    ad: $29.99 (50% off) \\n \\nGalaxy Star Projecto...\n",
       "Name: text, Length: 232, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('../data/3.pulledTweets-English.csv').drop_duplicates(subset=['text']).reset_index()\n",
    "df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     index  Unnamed: 0                      created_at            id  \\\n",
       "0        0           0  Sun May 02 07:59:00 +0000 2021  1.388765e+18   \n",
       "1        1           1  Sun May 02 22:11:00 +0000 2021  1.388979e+18   \n",
       "2        2           2  Sun May 02 14:21:00 +0000 2021  1.388861e+18   \n",
       "3        3           3  Mon May 03 23:59:58 +0000 2021  1.389369e+18   \n",
       "4        4           4  Mon May 03 23:59:52 +0000 2021  1.389369e+18   \n",
       "..     ...         ...                             ...           ...   \n",
       "227    251         251  Tue Apr 27 23:58:10 +0000 2021  1.387194e+18   \n",
       "228    252         252  Tue Apr 27 23:58:10 +0000 2021  1.387194e+18   \n",
       "229    253         253  Tue Apr 27 23:58:08 +0000 2021  1.387194e+18   \n",
       "230    254         254  Tue Apr 27 23:58:04 +0000 2021  1.387194e+18   \n",
       "231    255         255  Tue Apr 27 23:58:04 +0000 2021  1.387194e+18   \n",
       "\n",
       "                                                  text language  \n",
       "0    \"House of Cards\" has always hit differently ðŸŽ‚ ...       en  \n",
       "1    POWER A Enhanced Wireless Controller for Ninte...       en  \n",
       "2    Select 1st party Switch $39.99 physical/digita...       en  \n",
       "3    Silicone Eye Pad for Oculus Quest 2 Face Cushi...       en  \n",
       "4    Power Strip Surge Protector- 5 Outlets 3 USB P...       en  \n",
       "..                                                 ...      ...  \n",
       "227  Sometimes you get a third chance - Love's Thir...       en  \n",
       "228  Check out this Amazon deal: The Art of My Neig...       en  \n",
       "229  RT @bigtickHK: First Steps: How Upright Walkin...       en  \n",
       "230  RT @bigtickHK: The First Day of Spring by Nanc...       en  \n",
       "231  ad: $29.99 (50% off) \\n \\nGalaxy Star Projecto...       en  \n",
       "\n",
       "[232 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>Unnamed: 0</th>\n      <th>created_at</th>\n      <th>id</th>\n      <th>text</th>\n      <th>language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>Sun May 02 07:59:00 +0000 2021</td>\n      <td>1.388765e+18</td>\n      <td>\"House of Cards\" has always hit differently ðŸŽ‚ ...</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Sun May 02 22:11:00 +0000 2021</td>\n      <td>1.388979e+18</td>\n      <td>POWER A Enhanced Wireless Controller for Ninte...</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>Sun May 02 14:21:00 +0000 2021</td>\n      <td>1.388861e+18</td>\n      <td>Select 1st party Switch $39.99 physical/digita...</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>Mon May 03 23:59:58 +0000 2021</td>\n      <td>1.389369e+18</td>\n      <td>Silicone Eye Pad for Oculus Quest 2 Face Cushi...</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>Mon May 03 23:59:52 +0000 2021</td>\n      <td>1.389369e+18</td>\n      <td>Power Strip Surge Protector- 5 Outlets 3 USB P...</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>227</th>\n      <td>251</td>\n      <td>251</td>\n      <td>Tue Apr 27 23:58:10 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>Sometimes you get a third chance - Love's Thir...</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>228</th>\n      <td>252</td>\n      <td>252</td>\n      <td>Tue Apr 27 23:58:10 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>Check out this Amazon deal: The Art of My Neig...</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>229</th>\n      <td>253</td>\n      <td>253</td>\n      <td>Tue Apr 27 23:58:08 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>RT @bigtickHK: First Steps: How Upright Walkin...</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>230</th>\n      <td>254</td>\n      <td>254</td>\n      <td>Tue Apr 27 23:58:04 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>RT @bigtickHK: The First Day of Spring by Nanc...</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>231</th>\n      <td>255</td>\n      <td>255</td>\n      <td>Tue Apr 27 23:58:04 +0000 2021</td>\n      <td>1.387194e+18</td>\n      <td>ad: $29.99 (50% off) \\n \\nGalaxy Star Projecto...</td>\n      <td>en</td>\n    </tr>\n  </tbody>\n</table>\n<p>232 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df=df[df.text!='']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spams(file):\n",
    "    import pandas as pd\n",
    "    df=pd.read_csv(file).drop_duplicates(subset=['text']).reset_index()\n",
    "   \n",
    "    import re\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            txt = df.loc[i][\"text\"]\n",
    "            txt=re.sub(r'@[A-Z0-9a-z_:]+','',txt)#replace username-tags\n",
    "            txt=re.sub(r'^[RT]+','',txt)#replace RT-tags\n",
    "            txt = re.sub('https?://[A-Za-z0-9./]+','',txt)#replace URLs\n",
    "            txt=re.sub(\"[^a-zA-Z]\", \" \",txt)#replace hashtags\n",
    "            df.at[i,\"text\"]=txt\n",
    "        except:\n",
    "            df.at[i,\"text\"]=''\n",
    "            continue\n",
    "\n",
    "    df['spam']=spam_predict(df['text'])\n",
    "    df.to_csv('../data/4.Tweet data before clear spams.csv')\n",
    "    del df['Unnamed: 0']\n",
    "    df=df[df['spam']==0]\n",
    "    del df['spam']\n",
    "    df=df.where(df.text!='')\n",
    "    df.to_csv('../data/4.Tweet data after clear spams.csv')\n",
    "    return df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       House of Cards  has always hit differently   ...\n",
       "2      Select  st party Switch        physical digita...\n",
       "3      Silicone Eye Pad for Oculus Quest   Face Cushi...\n",
       "4      Power Strip Surge Protector    Outlets   USB P...\n",
       "6       Hi  You can turn off the microphone  When thi...\n",
       "                             ...                        \n",
       "227    Sometimes you get a third chance   Love s Thir...\n",
       "228    Check out this Amazon deal  The Art of My Neig...\n",
       "229      First Steps  How Upright Walking Made Us Hum...\n",
       "230      The First Day of Spring by Nancy Tucker    B...\n",
       "231    ad              off     Galaxy Star Projector ...\n",
       "Name: text, Length: 224, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "clean_spams('../data/3.pulledTweets-English.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python370jvsc74a57bd0398dc28c06ad810e77de546bbdfa897a6ee0b83e59a5207339dda01a7843e01d",
   "display_name": "Python 3.7.0 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}